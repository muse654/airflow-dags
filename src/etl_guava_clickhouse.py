import pandas as pd
from etl_base import ETLBase
from db_connectors import DatabaseConnector
import logging
import gc
from datetime import datetime
import re

logger = logging.getLogger(__name__)


class GuavaClickHouseETL(ETLBase):
    """Guava ClickHouseì—ì„œ Target ClickHouseë¡œ ì¦ë¶„ ë°ì´í„° ì´ê´€ (ìµœëŒ€ 10ë§Œ ê±´)"""
    
    def __init__(self, table_name, query=None, batch_size=500, max_rows=100000):
        super().__init__(f"Guava_ClickHouse_{table_name}")
        self.table_name = table_name
        self.base_query = query
        self.batch_size = batch_size
        self.max_rows = max_rows  # ìµœëŒ€ ì²˜ë¦¬ í–‰ ìˆ˜
        self.source_client = None
        self.target_client = None
        
        # OTel í…Œì´ë¸”ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ ë° íƒ€ì… ì •ì˜
        self.timestamp_info = {
            'otel_logs': {'column': 'Timestamp', 'type': 'DateTime64'},
            'otel_metrics_exponential_histogram': {'column': 'TimeUnix', 'type': 'Int64'},
            'otel_metrics_histogram': {'column': 'TimeUnix', 'type': 'Int64'},
            'otel_metrics_sum': {'column': 'TimeUnix', 'type': 'Int64'},
            'otel_metrics_gauge': {'column': 'TimeUnix', 'type': 'Int64'},
            'otel_metrics_summary': {'column': 'TimeUnix', 'type': 'Int64'},
            'otel_traces': {'column': 'Timestamp', 'type': 'DateTime64'}
        }
    
    def _get_last_timestamp(self, timestamp_col):
        """íƒ€ê²Ÿ í…Œì´ë¸”ì˜ ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤íƒ¬í”„ ì¡°íšŒ"""
        try:
            result = self.target_client.query(f"EXISTS TABLE `{self.table_name}`")
            table_exists = result.result_rows[0][0]
            
            if not table_exists:
                logger.info(f"íƒ€ê²Ÿ í…Œì´ë¸” `{self.table_name}` ì—†ìŒ")
                return None
            
            count_result = self.target_client.query(f"SELECT count() FROM `{self.table_name}`")
            row_count = count_result.result_rows[0][0]
            
            if row_count == 0:
                logger.info(f"íƒ€ê²Ÿ í…Œì´ë¸” `{self.table_name}` ë¹„ì–´ìˆìŒ (0í–‰)")
                return None
            
            # íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì¡°íšŒ
            ts_info = self.timestamp_info.get(self.table_name, {'column': 'Timestamp', 'type': 'DateTime64'})
            
            if ts_info['type'] == 'Int64':
                # TimeUnix - ì´ë¯¸ ë‚˜ë…¸ì´ˆ ì •ìˆ˜
                result = self.target_client.query(
                    f"SELECT max(`{timestamp_col}`) FROM `{self.table_name}`"
                )
                last_ts = result.result_rows[0][0]
                
                if last_ts:
                    logger.info(f"ë§ˆì§€ë§‰ {timestamp_col} (Int64): {last_ts} ({row_count:,}í–‰)")
                    return int(last_ts)
            else:
                # DateTime64 - ë¬¸ìì—´ë¡œ ë°˜í™˜
                result = self.target_client.query(
                    f"SELECT toString(max(`{timestamp_col}`)) FROM `{self.table_name}`"
                )
                last_ts = result.result_rows[0][0]
                
                if last_ts:
                    logger.info(f"ë§ˆì§€ë§‰ {timestamp_col} (DateTime64): {last_ts} ({row_count:,}í–‰)")
                    return last_ts
            
            return None
            
        except Exception as e:
            logger.warning(f"ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤íƒ¬í”„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def _build_where_clause(self, timestamp_col, timestamp_type, last_value):
        """WHERE ì¡°ê±´ ìƒì„±"""
        if not last_value:
            logger.info("âš ï¸ ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜: íƒ€ê²Ÿ í…Œì´ë¸”ì´ ë¹„ì–´ìˆìŒ")
            return ""
        
        logger.info(f"âœ… ì¦ë¶„ ë§ˆì´ê·¸ë ˆì´ì…˜: {timestamp_col} > {last_value} (íƒ€ì…: {timestamp_type})")
        
        # Int64 íƒ€ì… (ì´ë¯¸ ë‚˜ë…¸ì´ˆ ì •ìˆ˜)
        if timestamp_type == 'Int64' and isinstance(last_value, (int, float)):
            return f"WHERE `{timestamp_col}` > {int(last_value)}"
        
        # DateTime64 íƒ€ì… (ë¬¸ìì—´)
        if timestamp_type == 'DateTime64' and isinstance(last_value, str):
            # íƒ€ì„ì¡´ ì œê±°
            timestamp_str = last_value.replace('T', ' ')
            for tz in ['+09:00', '+00:00', 'Z', '-00:00', '+01:00', '-01:00']:
                timestamp_str = timestamp_str.replace(tz, '')
            timestamp_str = timestamp_str.strip()
            
            logger.info(f"ì •ë¦¬ëœ íƒ€ì„ìŠ¤íƒ¬í”„: '{timestamp_str}'")
            return f"WHERE `{timestamp_col}` > '{timestamp_str}'"
        
        # Int64ì¸ë° ë¬¸ìì—´ë¡œ ì™”ì„ ë•Œ
        if timestamp_type == 'Int64' and isinstance(last_value, str):
            try:
                # íƒ€ì„ì¡´ ì œê±°
                timestamp_str = last_value.replace('T', ' ')
                for tz in ['+09:00', '+00:00', 'Z', '-00:00', '+01:00', '-01:00']:
                    timestamp_str = timestamp_str.replace(tz, '')
                timestamp_str = timestamp_str.strip()
                
                # Pythonì—ì„œ ë‚˜ë…¸ì´ˆ ê³„ì‚°
                match = re.match(r'(\d{4})-(\d{2})-(\d{2})\s+(\d{2}):(\d{2}):(\d{2})\.?(\d{0,9})?', timestamp_str)
                if match:
                    year, month, day, hour, minute, second, nanosec = match.groups()
                    nanosec = (nanosec or '0').ljust(9, '0')[:9]
                    
                    dt_obj = datetime(int(year), int(month), int(day), int(hour), int(minute), int(second))
                    timestamp_seconds = int(dt_obj.timestamp())
                    timestamp_nanos = timestamp_seconds * 1000000000 + int(nanosec)
                    
                    logger.info(f"ê³„ì‚°ëœ ë‚˜ë…¸ì´ˆ: {timestamp_nanos}")
                    return f"WHERE `{timestamp_col}` > {timestamp_nanos}"
                else:
                    logger.error(f"íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹± ì‹¤íŒ¨: {timestamp_str}")
                    return ""
            except Exception as e:
                logger.error(f"íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜ ì‹¤íŒ¨: {e}")
                return ""
        
        logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì… ì¡°í•©: {timestamp_type}, {type(last_value)}")
        return ""
    
    def run(self):
        """ETL íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ìµœëŒ€ 10ë§Œ ê±´)"""
        try:
            self.start_time = datetime.now()
            logger.info(f"[{self.job_name}] ETL ì‹œì‘: {self.start_time}")
            logger.info(f"âš ï¸ ìµœëŒ€ ì²˜ë¦¬ í–‰ ìˆ˜: {self.max_rows:,}ê±´")
            
            # í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
            self.source_client = DatabaseConnector.get_clickhouse_client('GUAVA')
            self.target_client = DatabaseConnector.get_clickhouse_client('TARGET')
            
            # í…Œì´ë¸” êµ¬ì¡° ë³µì œ
            self._create_otel_table_from_source(self.table_name)
            
            # ì¦ë¶„ ì¡°ê±´ í™•ì¸
            ts_info = self.timestamp_info.get(self.table_name, {'column': 'Timestamp', 'type': 'DateTime64'})
            timestamp_col = ts_info['column']
            timestamp_type = ts_info['type']
            
            last_value = self._get_last_timestamp(timestamp_col)
            
            # WHERE ì¡°ê±´ êµ¬ì„±
            where_clause = self._build_where_clause(timestamp_col, timestamp_type, last_value)
            
            if not where_clause and last_value:
                logger.error(f"âŒ WHERE ì¡°ê±´ ìƒì„± ì‹¤íŒ¨, '{self.table_name}' í…Œì´ë¸” ìŠ¤í‚µ")
                return
            
            # ì „ì²´ í–‰ ìˆ˜ í™•ì¸
            count_query = f"SELECT count() FROM `{self.table_name}` {where_clause}"
            logger.info(f"Count ì¿¼ë¦¬: {count_query}")
            
            count_result = self.source_client.query(count_query)
            total_rows = count_result.result_rows[0][0]
            
            # 10ë§Œ ê±´ìœ¼ë¡œ ì œí•œ
            if total_rows > self.max_rows:
                logger.warning(f"âš ï¸ ì „ì²´ {total_rows:,}í–‰ ì¤‘ {self.max_rows:,}í–‰ë§Œ ì²˜ë¦¬")
                total_rows = self.max_rows
            else:
                logger.info(f"ì´ {total_rows:,}í–‰ ì²˜ë¦¬ ì˜ˆì • (ë°°ì¹˜ í¬ê¸°: {self.batch_size:,})")
            
            if total_rows == 0:
                logger.info("âœ… ì²˜ë¦¬í•  ì‹ ê·œ ë°ì´í„° ì—†ìŒ")
                return
            
            # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬
            self._process_batches(where_clause, timestamp_col, total_rows)
            
            self.end_time = datetime.now()
            duration = (self.end_time - self.start_time).total_seconds()
            logger.info(f"[{self.job_name}] âœ… ETL ì™„ë£Œ: {duration:.2f}ì´ˆ ì†Œìš”")
            
        except Exception as e:
            logger.error(f"[{self.job_name}] âŒ ETL ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            raise
        finally:
            if self.source_client:
                try:
                    self.source_client.close()
                except:
                    pass
            if self.target_client:
                try:
                    self.target_client.close()
                except:
                    pass
    
    def _process_batches(self, where_clause, timestamp_col, total_rows):
        """ë°°ì¹˜ë³„ ë°ì´í„° ì²˜ë¦¬ (ORDER BY ì œê±°ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)"""
        offset = 0
        batch_num = 0
        total_processed = 0
        consecutive_failures = 0
        max_consecutive_failures = 3
        
        while offset < total_rows:
            batch_num += 1
            
            # ë‚¨ì€ í–‰ ìˆ˜ ê³„ì‚°
            remaining = total_rows - offset
            current_batch_size = min(self.batch_size, remaining)
            
            # Extract (ORDER BY ì œê±° - ë©”ëª¨ë¦¬ ì ˆì•½)
            # WHERE ì¡°ê±´ìœ¼ë¡œ ì´ë¯¸ íƒ€ì„ìŠ¤íƒ¬í”„ í•„í„°ë§ë˜ë¯€ë¡œ ORDER BY ë¶ˆí•„ìš”
            query = f"SELECT * FROM `{self.table_name}` {where_clause} LIMIT {current_batch_size} OFFSET {offset}"
            
            logger.info(f"ë°°ì¹˜ {batch_num}: {offset:,} ~ {min(offset + current_batch_size, total_rows):,}")
            
            try:
                df_batch = self.source_client.query_df(query)
                consecutive_failures = 0
            except Exception as e:
                consecutive_failures += 1
                error_msg = str(e)
                
                # ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬ ê°ì§€
                if 'MEMORY_LIMIT_EXCEEDED' in error_msg:
                    logger.error(f"âš ï¸ ë©”ëª¨ë¦¬ ë¶€ì¡± ê°ì§€, ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ ì‹œë„")
                    
                    # ë°°ì¹˜ í¬ê¸°ë¥¼ ì ˆë°˜ìœ¼ë¡œ
                    if current_batch_size > 100:
                        smaller_batch = current_batch_size // 2
                        query = f"SELECT * FROM `{self.table_name}` {where_clause} LIMIT {smaller_batch} OFFSET {offset}"
                        
                        gc.collect()
                        
                        try:
                            logger.info(f"ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ: {current_batch_size} â†’ {smaller_batch}")
                            df_batch = self.source_client.query_df(query)
                            current_batch_size = smaller_batch  # ì´í›„ ë°°ì¹˜ë„ ì‘ê²Œ
                            self.batch_size = smaller_batch
                            consecutive_failures = 0
                            logger.info(f"âœ… ì‘ì€ ë°°ì¹˜ë¡œ ì„±ê³µ")
                        except Exception as e2:
                            logger.error(f"ë°°ì¹˜ {batch_num} ì¶•ì†Œ í›„ì—ë„ ì‹¤íŒ¨: {e2}")
                            # ìŠ¤í‚µí•˜ê³  ë‹¤ìŒìœ¼ë¡œ
                            offset += smaller_batch
                            continue
                    else:
                        logger.error(f"ë°°ì¹˜ í¬ê¸°ê°€ ì´ë¯¸ ìµœì†Œê°’({current_batch_size}), ìŠ¤í‚µ")
                        offset += current_batch_size
                        continue
                else:
                    logger.error(f"ë°°ì¹˜ {batch_num} ì¶”ì¶œ ì‹¤íŒ¨ ({consecutive_failures}/{max_consecutive_failures}): {e}")
                
                if consecutive_failures >= max_consecutive_failures:
                    logger.error(f"ì—°ì† {max_consecutive_failures}ë²ˆ ì‹¤íŒ¨, ETL ì¤‘ë‹¨")
                    logger.info(f"âœ… {total_processed:,}ê±´ê¹Œì§€ ì²˜ë¦¬ ì™„ë£Œ")
                    return  # ì¤‘ë‹¨í•˜ì§€ ì•Šê³  ì¢…ë£Œ
                
                gc.collect()
                
                try:
                    logger.info(f"ë°°ì¹˜ {batch_num} ì¬ì‹œë„...")
                    df_batch = self.source_client.query_df(query)
                    consecutive_failures = 0
                except Exception as retry_e:
                    logger.error(f"ë°°ì¹˜ {batch_num} ì¬ì‹œë„ ì‹¤íŒ¨: {retry_e}")
                    offset += current_batch_size
                    continue
            
            if len(df_batch) == 0:
                logger.info(f"ë°°ì¹˜ {batch_num}: ë°ì´í„° ì—†ìŒ, ì¢…ë£Œ")
                break
            
            # Transform
            try:
                df_transformed = self.transform(df_batch)
            except Exception as e:
                logger.error(f"ë°°ì¹˜ {batch_num} ë³€í™˜ ì‹¤íŒ¨: {e}")
                offset += current_batch_size
                continue
            
            # Load
            try:
                self._load_batch(df_transformed)
            except Exception as e:
                logger.error(f"ë°°ì¹˜ {batch_num} ì ì¬ ì‹¤íŒ¨: {e}")
                # ì ì¬ ì‹¤íŒ¨ëŠ” ì¤‘ìš”í•˜ì§€ë§Œ ê³„ì† ì§„í–‰
                logger.warning(f"ë°°ì¹˜ {batch_num} ìŠ¤í‚µí•˜ê³  ê³„ì† ì§„í–‰")
                offset += current_batch_size
                continue
            
            total_processed += len(df_batch)
            progress = (total_processed / total_rows * 100) if total_rows > 0 else 0
            
            if batch_num % 10 == 0 or batch_num == 1:
                logger.info(f"ğŸ“Š ì§„í–‰ë¥ : {total_processed:,}/{total_rows:,} ({progress:.1f}%) - ë°°ì¹˜ {batch_num}")
            
            del df_batch
            del df_transformed
            gc.collect()
            
            offset += current_batch_size
        
        logger.info(f"âœ… ì´ {total_processed:,}ê±´ ì²˜ë¦¬ ì™„ë£Œ")
    
    def extract(self):
        pass
    
    def transform(self, data):
        """ë°ì´í„° ë³€í™˜ (DateTime íƒ€ì… ì²˜ë¦¬ ê°•í™”)"""
        if len(data) == 0:
            return data
        
        # 1. ë‹¨ì¼ datetime ì»¬ëŸ¼ ì²˜ë¦¬
        datetime_columns = ['StartTimeUnix', 'TimeUnix', 'Timestamp']
        for col in datetime_columns:
            if col in data.columns:
                if 'datetime64' in str(data[col].dtype):
                    data[col] = pd.to_datetime(data[col])
                    logger.debug(f"'{col}': datetime64 ìœ ì§€")
        
        # 2. ë°°ì—´ ì»¬ëŸ¼ ì²˜ë¦¬ - Events.*, Exemplars.*, Links.* ë“±
        array_columns = [col for col in data.columns if '.' in col]
        
        for col in array_columns:
            if col not in data.columns:
                continue
            
            # ë°°ì—´ ì•ˆì— datetimeì´ ìˆëŠ” ê²½ìš°
            if 'Timestamp' in col or 'TimeUnix' in col:
                def convert_datetime_array(val):
                    if not isinstance(val, list):
                        return []
                    
                    result = []
                    for item in val:
                        if pd.notna(item):
                            try:
                                # numpy.datetime64 â†’ pandas Timestamp
                                if hasattr(item, '__class__') and 'datetime64' in str(type(item)):
                                    result.append(pd.Timestamp(item))
                                elif isinstance(item, str):
                                    result.append(pd.to_datetime(item))
                                elif hasattr(item, 'to_pydatetime'):
                                    result.append(pd.Timestamp(item))
                                else:
                                    result.append(pd.Timestamp(item))
                            except Exception as e:
                                logger.warning(f"ë°°ì—´ í•­ëª© ë³€í™˜ ì‹¤íŒ¨ ({col}): {e}")
                                pass
                    return result
                
                data[col] = data[col].apply(convert_datetime_array)
                logger.debug(f"'{col}': datetime ë°°ì—´ ë³€í™˜ ì™„ë£Œ")
            
            # ì¼ë°˜ ë°°ì—´ (ë¹ˆ ë°°ì—´ë¡œ ì²˜ë¦¬)
            else:
                data[col] = data[col].apply(lambda x: x if isinstance(x, list) else [])
        
        # 3. Map íƒ€ì… ì»¬ëŸ¼
        map_columns = ['ResourceAttributes', 'ScopeAttributes', 'Attributes']
        for col in map_columns:
            if col in data.columns:
                data[col] = data[col].apply(lambda x: x if isinstance(x, dict) else {})
        
        # 4. ì¼ë°˜ NaN ì²˜ë¦¬
        for col in data.columns:
            try:
                if col in datetime_columns or col in array_columns or col in map_columns:
                    continue
                
                if data[col].dtype == 'object':
                    data[col] = data[col].fillna('')
                elif data[col].dtype in ['int64', 'int32', 'uint64', 'uint32', 'uint16', 'uint8']:
                    data[col] = data[col].fillna(0)
                elif data[col].dtype in ['float64', 'float32']:
                    data[col] = data[col].fillna(0.0)
            except:
                pass
        
        return data
    
    def _load_batch(self, data):
        """ë°°ì¹˜ ë°ì´í„° ì ì¬"""
        if len(data) == 0:
            return
        
        try:
            self.target_client.insert_df(self.table_name, data)
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì ì¬ ì‹¤íŒ¨: {e}")
            raise
    
    def load(self, data):
        pass
    
    def _create_otel_table_from_source(self, table_name):
        """ì†ŒìŠ¤ ClickHouseì˜ í…Œì´ë¸” êµ¬ì¡°ë¥¼ ë³µì œ"""
        try:
            show_create_result = self.source_client.query(f"SHOW CREATE TABLE `{table_name}`")
            create_statement = show_create_result.result_rows[0][0]
            
            create_statement = create_statement.replace('CREATE TABLE', 'CREATE TABLE IF NOT EXISTS', 1)
            
            self.target_client.command(create_statement)
            logger.info(f"âœ… í…Œì´ë¸” `{table_name}` ìƒì„±/í™•ì¸ ì™„ë£Œ")
        except Exception as e:
            if 'already exists' not in str(e).lower():
                logger.warning(f"í…Œì´ë¸” ìƒì„± ì‹œë„ ì¤‘ ê²½ê³ : {e}")
    
    def __del__(self):
        if self.source_client:
            try:
                self.source_client.close()
            except:
                pass
        if self.target_client:
            try:
                self.target_client.close()
            except:
                pass